{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI EDA\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nibabel as nib\n",
    "from ipywidgets import interact\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mri_case,mri_type=\"t2_tse_fs_cor\"):\n",
    "    path = f\"../raw_data/nii_files/{mri_type}/{mri_case}.nii\"\n",
    "    return nib.load(path).get_fdata()\n",
    "\n",
    "def visualize_slices(mri_list,preprocess_slices=None):\n",
    "    if preprocess_slices !=None:\n",
    "        mri_list_processed = list(map(preprocess_slices,mri_list))\n",
    "        n_slices = mri_list_processed[0].shape[2]\n",
    "        starting_point = mri_list[0].shape[2]//2 - n_slices//2\n",
    "    else:\n",
    "        mri_list_processed = None\n",
    "    cmap = plt.cm.winter\n",
    "    # Get the colormap colors\n",
    "    my_cmap = cmap(np.arange(cmap.N))\n",
    "    my_cmap[:, -1] = np.linspace(0, 1, cmap.N)\n",
    "    my_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "    # Function to visualize a single slice\n",
    "    def show_slice(mri, slice_number):\n",
    "        if mri_list_processed == None:\n",
    "            plt.imshow(mri_list[mri][:, :, slice_number], cmap='gray')\n",
    "            \n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Create a figure with 2 subplots\n",
    "            axes[0].imshow(mri_list[mri][:, :, slice_number], cmap='gray')\n",
    "            axes[0].set_title(f'Not Preprocessed')\n",
    "            if (starting_point <= slice_number) and (starting_point + n_slices > slice_number):\n",
    "                axes[1].imshow(mri_list_processed[mri][:, :, slice_number-starting_point], cmap='gray')\n",
    "                axes[1].set_title(f'Preprocessed')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    interact(show_slice, mri=(0, len(mri_list) - 1), slice_number=(0, mri_list[0].shape[2] - 1))\n",
    "\n",
    "\n",
    "def resize_and_crop_3d_image(image, new_size=(384, 384), crop_size= (112, 112, 6)):\n",
    "    resize_transform = transforms.Resize(new_size)\n",
    "\n",
    "    # Process each slice\n",
    "    resized_slices = []\n",
    "    for slice_idx in range(image.shape[2]):\n",
    "        # Extract the slice and add a channel dimension\n",
    "        slice = image[:, :, slice_idx]\n",
    "        slice = torch.tensor(slice).unsqueeze(0)  # Add a channel dimension\n",
    "        resized_slice = resize_transform(slice)\n",
    "        resized_slices.append(resized_slice.squeeze(0).numpy())\n",
    "\n",
    "    resized_image = np.stack(resized_slices, axis=2)\n",
    "    center = np.array(resized_image.shape) // 2\n",
    "    cropped_image = resized_image[\n",
    "        center[0]-crop_size[0]//2 : center[0]+crop_size[0]//2,\n",
    "        center[1]-crop_size[1]//2 : center[1]+crop_size[1]//2,\n",
    "        center[2]-crop_size[2]//2 : center[2]+crop_size[2]//2\n",
    "    ]\n",
    "\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2050b634aa7d488495587ceaf1e4fb45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='mri', max=0), IntSlider(value=9, description='slice_numb…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2_tse_cor = load_data(\"7729409\",\"t2_tse_cor\")\n",
    "visualize_slices([t2_tse_cor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c83de7ea59524e9985eb7b163385a3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='mri', max=0), IntSlider(value=7, description='slice_numb…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case_879 = load_data(\"8797386\")\n",
    "visualize_slices([case_879])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79ab7a4793d44b2e910f21632da74d4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='mri', max=0), IntSlider(value=7, description='slice_numb…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "resized_and_cropped_image = resize_and_crop_3d_image(case_879)\n",
    "visualize_slices([case_879],resize_and_crop_3d_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/train_data.csv\")\n",
    "cases = train_data[\"MRI_Case_ID\"][10:30]\n",
    "mri = [load_data(mri_case) for mri_case in cases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/azureml_py38/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9da69dcb3cc4ec88e8a33868aaf07e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=9, description='mri', max=19), IntSlider(value=9, description='slice_num…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_slices(mri,resize_and_crop_3d_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
