{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MRI EDA\n",
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nibabel as nib\n",
    "from ipywidgets import interact\n",
    "from matplotlib.colors import ListedColormap\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(mri_case,mri_type=\"t2_tse_fs_cor\"):\n",
    "    path = f\"../raw_data/nii_files/{mri_type}/{mri_case}.nii\"\n",
    "    return nib.load(path).get_fdata()\n",
    "    \n",
    "def visualize_slices(mri_list,preprocess_slices=None):\n",
    "    if preprocess_slices !=None:\n",
    "        mri_list_processed = list(map(preprocess_slices,mri_list))\n",
    "        n_slices = mri_list_processed[0].shape[2]\n",
    "    else:\n",
    "        mri_list_processed = None\n",
    "    cmap = plt.cm.winter\n",
    "    # Get the colormap colors\n",
    "    my_cmap = cmap(np.arange(cmap.N))\n",
    "    my_cmap[:, -1] = np.linspace(0, 1, cmap.N)\n",
    "    my_cmap = ListedColormap(my_cmap)\n",
    "\n",
    "    # Function to visualize a single slice\n",
    "    def show_slice(mri, slice_number):\n",
    "        if mri_list_processed == None:\n",
    "            plt.imshow(mri_list[mri][:, :, slice_number], cmap='gray')\n",
    "            \n",
    "        else:\n",
    "            starting_point = mri_list[mri].shape[2]//2 - n_slices//2\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 5))  # Create a figure with 2 subplots\n",
    "            axes[0].imshow(mri_list[mri][:, :, slice_number], cmap='gray')\n",
    "            axes[0].set_title(f'Not Preprocessed')\n",
    "            if (starting_point <= slice_number) and (starting_point + n_slices > slice_number):\n",
    "                axes[1].imshow(mri_list_processed[mri][:, :, slice_number-starting_point], cmap='gray')\n",
    "                axes[1].set_title(f'Preprocessed')\n",
    "        plt.show()\n",
    "\n",
    "    \n",
    "    interact(show_slice, mri=(0, len(mri_list) - 1), slice_number=(0, mri_list[0].shape[2] - 1))\n",
    "\n",
    "\n",
    "def resize_and_crop_3d_image_cor(image, new_size=(384, 384), crop_size= (112, 112, 6)):\n",
    "    resize_transform = transforms.Resize(new_size)\n",
    "\n",
    "    # Process each slice\n",
    "    resized_slices = []\n",
    "    for slice_idx in range(image.shape[2]):\n",
    "        # Extract the slice and add a channel dimension\n",
    "        slice = image[:, :, slice_idx]\n",
    "        slice = torch.tensor(slice).unsqueeze(0)  # Add a channel dimension\n",
    "        resized_slice = resize_transform(slice)\n",
    "        resized_slices.append(resized_slice.squeeze(0).numpy())\n",
    "\n",
    "    resized_image = np.stack(resized_slices, axis=2)\n",
    "    center = np.array(resized_image.shape) // 2\n",
    "    cropped_image = resized_image[\n",
    "        center[0]-crop_size[0]//2 : center[0]+crop_size[0]//2,\n",
    "        center[1]-crop_size[1]//2 : center[1]+crop_size[1]//2,\n",
    "        center[2]-crop_size[2]//2 : center[2]+crop_size[2]//2\n",
    "    ]\n",
    "\n",
    "    return cropped_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_crop_3d_image_seg(image, new_size=(384, 384), crop_size= (150, 150, 8)): # 112, 112, 6\n",
    "    resize_transform = transforms.Resize(new_size)\n",
    "\n",
    "    # Process each slice\n",
    "    resized_slices = []\n",
    "    for slice_idx in range(image.shape[2]):\n",
    "        # Extract the slice and add a channel dimension\n",
    "        slice = image[:, :, slice_idx]\n",
    "        slice = torch.tensor(slice).unsqueeze(0)  # Add a channel dimension\n",
    "        resized_slice = resize_transform(slice)\n",
    "        resized_slices.append(resized_slice.squeeze(0).numpy())\n",
    "\n",
    "    resized_image = np.stack(resized_slices, axis=2)\n",
    "    center = np.array(resized_image.shape) // 2\n",
    "    cropped_image = resized_image[\n",
    "        center[0]-crop_size[0]//2 + 20 : center[0]+crop_size[0]//2 + 20,\n",
    "        center[1]-crop_size[1]//2 - 20 : center[1]+crop_size[1]//2 - 20,\n",
    "        center[2]-crop_size[2]//2 : center[2]+crop_size[2]//2\n",
    "    ]\n",
    "\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef45704b4cb043e39e6e8311c5a8f178",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='mri', max=0), IntSlider(value=9, description='slice_numb…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t2_tse_cor = load_data(\"7729409\",\"t2_tse_cor\")\n",
    "visualize_slices([t2_tse_cor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18d5a89f1ea840828774761d84e67fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='mri', max=0), IntSlider(value=7, description='slice_numb…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "case_879 = load_data(\"8797386\")\n",
    "resized_and_cropped_image = resize_and_crop_3d_image_cor(case_879)\n",
    "visualize_slices([case_879],resize_and_crop_3d_image_cor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"../data/train_data.csv\")\n",
    "cases = train_data[\"MRI_Case_ID\"][17:25]\n",
    "mri = [load_data(mri_case,\"t1_tse_sag\") for mri_case in cases]\n",
    "# Sagital auf 8 erhöhen 140 x 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b8b16e03314512a3a724d6387066bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=3, description='mri', max=7), IntSlider(value=9, description='slice_numb…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "visualize_slices(mri,resize_and_crop_3d_image_seg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
